{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from logging import INFO, ERROR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vgg16\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "VECTOR_DB_PATH = \"./vectordatabase_beer_1\"\n",
    "\n",
    "def init_vectordb():\n",
    "    try:\n",
    "        chromadb.PersistentClient(path=VECTOR_DB_PATH)\n",
    "        print(\"Setup vectordb OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"Setup vectordb failed! - {str(e)}\")\n",
    "    \n",
    "def get_chroma_collection():\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=VECTOR_DB_PATH)\n",
    "        collection = client.get_or_create_collection(name=\"beer_similarity_check_1\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "        \n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        print(f\"Get vectordb client error {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "def beer_detection(image_path,detector):\n",
    "        image = cv2.imread(image_path)\n",
    "        results = detector(image, conf=0.25)\n",
    "        crop_list=[]\n",
    "        for result in results:\n",
    "                boxes = result.boxes.xyxy.cpu().numpy()\n",
    "                classes = result.boxes.cls.cpu().numpy()\n",
    "                scores = result.boxes.conf.cpu().numpy()\n",
    "                for i, (box, cls, score) in enumerate(zip(boxes, classes, scores)):\n",
    "                        x1, y1, x2, y2 = map(int, box)\n",
    "                        crop_img = image[y1:y2, x1:x2]\n",
    "                        transform = transforms.ToTensor()\n",
    "                        crop_list.append(transform(crop_img))\n",
    "        return crop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (224, 224)\n",
    "\n",
    "def process_image(image_path, normalize=True):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    transform_list = [\n",
    "        transforms.Resize(TARGET_SIZE),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "    if normalize:\n",
    "        transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    \n",
    "    transform = transforms.Compose(transform_list)\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "class EmbeddingModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        model = vgg16(pretrained=True)\n",
    "        self.embedding_model = nn.Sequential(*list(model.children())[:-1])  # Remove the classifier\n",
    "        self.embedding_model.eval()\n",
    "        \n",
    "    def embed(self, img_path, normalize=True):\n",
    "        \"\"\"Embed the image using the model.\"\"\"\n",
    "        if isinstance(img_path, str):\n",
    "            assert os.path.exists(img_path), f\"Image path {img_path} not found!\"\n",
    "            img_tensor = process_image(img_path, normalize)\n",
    "        else:\n",
    "            img_tensor = img_path\n",
    "            if normalize:\n",
    "                img_tensor = self._normalize(img_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            representation = self.embedding_model(img_tensor)\n",
    "        \n",
    "        return representation.flatten().numpy()\n",
    "    \n",
    "    def _normalize(self, img_tensor):\n",
    "        \"\"\"Normalize the image tensor.\"\"\"\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return normalize(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(image_path, top_k=5):\n",
    "    model = YOLO('runs/detect/beer7/weights/best.pt')\n",
    "    beers=beer_detection(image_path,model)\n",
    "    init_vectordb()\n",
    "    vectordb_coll=get_chroma_collection()\n",
    "    embedding_model = EmbeddingModel()\n",
    "    results=[]\n",
    "    for beer in beers:\n",
    "        embed_feature=embedding_model.embed(beer).tolist()\n",
    "        result = vectordb_coll.query(query_embeddings=[embed_feature],n_results=top_k)\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 9 Beers, 8.5ms\n",
      "Speed: 1.5ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Setup vectordb OK\n"
     ]
    }
   ],
   "source": [
    "image_path=\"dataset/train/874a993ebf4f444a3e6c21c9b172143a20211221120302.5637770saigonbeer.jpg\"\n",
    "results=main(image_path,top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 bia Sai_gon\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_brands = []\n",
    "\n",
    "# Lặp qua từng dictionary trong data\n",
    "for d in results:\n",
    "    # Lấy ra các IDs từ dictionary hiện tại\n",
    "    ids = d['ids'][0]  # ids là một list chứa các ID\n",
    "    \n",
    "    # List để lưu trữ các brand của từng ID\n",
    "    id_brands = []\n",
    "    \n",
    "    # Lặp qua từng metadata_list trong metadatas của dictionary hiện tại\n",
    "    for metadata_list in d['metadatas']:\n",
    "        # List để lưu trữ các brand từng metadata_list\n",
    "        metadata_brands = []\n",
    "        \n",
    "        # Lặp qua từng metadata trong metadata_list\n",
    "        for metadata in metadata_list:\n",
    "            # Lấy ra brand từ metadata và thêm vào list metadata_brands\n",
    "            metadata_brands.append(metadata['brand'])\n",
    "        \n",
    "        # Thêm danh sách các brand của metadata_list vào id_brands\n",
    "        id_brands.extend(metadata_brands)\n",
    "    \n",
    "    # Lưu trữ tất cả các brand của dictionary hiện tại\n",
    "    all_brands.append((ids, id_brands))\n",
    "\n",
    "brand_dict = {\n",
    "  \"Heineken\": 0,\n",
    "  \"Tiger\": 0,\n",
    "  \"Bia_Viet\": 0,\n",
    "  \"Larue\": 0,\n",
    "  \"Bivina\": 0,\n",
    "  \"Edelweiss\": 0,\n",
    "  \"Strongbow\": 0,\n",
    "  \"Sai_gon\": 0,\n",
    "  \"333\": 0,\n",
    "  \"Huda\": 0,\n",
    "}\n",
    "for ids, brands in all_brands:\n",
    "    # Đếm số lần xuất hiện của mỗi brand trong similarity\n",
    "    brand_counter = Counter(brands)\n",
    "    most_common_brand = brand_counter.most_common(1)[0][0]\n",
    "    brand_dict[f\"{most_common_brand}\"]+=1\n",
    "\n",
    "for key, value in brand_dict.items():\n",
    "    if value != 0:\n",
    "        print(f\"{value} bia {key}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
